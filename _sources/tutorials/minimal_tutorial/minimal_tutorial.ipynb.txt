{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6638863",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Cremi Example\n",
    "This tutorial goes through a few common ML tasks using the [cremi dataset](https://cremi.org/data/)\n",
    "and a *2.5D U-Net* (It takes a shape (5, 156, 156) input and generates a shape (1, 64, 64) output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba54f4",
   "metadata": {},
   "source": [
    "## Introduction and overview\n",
    "\n",
    "In this tutorial we will cover a few basic ML tasks using the DaCapo toolbox. We will:\n",
    "\n",
    "- Prepare a dataloader for the CREMI dataset\n",
    "- Train a simple 2D U-Net for both instance and semantic segmentation\n",
    "- Visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b8b5d",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "If you have not already done so, you will need to install DaCapo. You can do this\n",
    "by first creating a new environment and then installing the DaCapo Toolbox.\n",
    "\n",
    "I highly recommend using [uv](https://docs.astral.sh/uv/) for environment management, but there are many tools to choose from.\n",
    "\n",
    "```bash\n",
    "uv init\n",
    "uv add git+https://github.com/pattonw/dacapo-toolbox.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600512b3",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "DaCapo works with zarr, so we will download [CREMI Sample A](https://cremi.org/static/data/sample_A%2B_20160601.hdf)\n",
    "and save it as a zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "import wget\n",
    "from pathlib import Path\n",
    "import dask\n",
    "\n",
    "if not Path(\"_static/minimal_tutorial\").exists():\n",
    "    Path(\"_static/minimal_tutorial\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dask.config.set(scheduler=\"single-threaded\")\n",
    "\n",
    "# Download some cremi data\n",
    "# immediately convert it to zarr for convenience\n",
    "if not Path(\"sample_A_20160501.hdf\").exists():\n",
    "    wget.download(\n",
    "        \"https://cremi.org/static/data/sample_C_20160501.hdf\", \"sample_C_20160501.hdf\"\n",
    "    )\n",
    "    wget.download(\n",
    "        \"https://cremi.org/static/data/sample_A_20160501.hdf\", \"sample_A_20160501.hdf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d38c5d",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "We will use the [funlib.persistence](github.com/funkelab/funlib.persistence) library to interface with zarr. This library adds support for units, voxel size, and axis names along with the ability to query our data based on a `Roi` object describing a specific rectangular piece of data. This is especially useful in a microscopy context where you regularly need to chunk your data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from funlib.persistence import prepare_ds, open_ds\n",
    "import h5py\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"cremi.zarr/train/raw\").exists():\n",
    "    test = h5py.File(\"sample_C_20160501.hdf\", \"r\")\n",
    "    raw_data = test[\"volumes/raw\"][:]\n",
    "    labels_data = test[\"volumes/labels/neuron_ids\"][:]\n",
    "    test_raw = prepare_ds(\n",
    "        \"cremi.zarr/test/raw\",\n",
    "        raw_data.shape,\n",
    "        voxel_size=(40, 4, 4),\n",
    "        dtype=raw_data.dtype,\n",
    "        axis_names=[\"z\", \"y\", \"x\"],\n",
    "        units=[\"nm\", \"nm\", \"nm\"],\n",
    "    )\n",
    "    test_raw[test_raw.roi] = raw_data\n",
    "    test_labels = prepare_ds(\n",
    "        \"cremi.zarr/test/labels\",\n",
    "        labels_data.shape,\n",
    "        voxel_size=(40, 4, 4),\n",
    "        dtype=labels_data.dtype,\n",
    "        axis_names=[\"z\", \"y\", \"x\"],\n",
    "        units=[\"nm\", \"nm\", \"nm\"],\n",
    "    )\n",
    "    test_labels[test_labels.roi] = labels_data\n",
    "    train = h5py.File(\"sample_A_20160501.hdf\", \"r\")\n",
    "    raw_data = train[\"volumes/raw\"][:]\n",
    "    labels_data = train[\"volumes/labels/neuron_ids\"][:]\n",
    "    train_raw = prepare_ds(\n",
    "        \"cremi.zarr/train/raw\",\n",
    "        raw_data.shape,\n",
    "        voxel_size=(40, 4, 4),\n",
    "        dtype=raw_data.dtype,\n",
    "        axis_names=[\"z\", \"y\", \"x\"],\n",
    "        units=[\"nm\", \"nm\", \"nm\"],\n",
    "    )\n",
    "    train_raw[train_raw.roi] = raw_data\n",
    "    train_labels = prepare_ds(\n",
    "        \"cremi.zarr/train/labels\",\n",
    "        labels_data.shape,\n",
    "        voxel_size=(40, 4, 4),\n",
    "        dtype=labels_data.dtype,\n",
    "        axis_names=[\"z\", \"y\", \"x\"],\n",
    "        units=[\"nm\", \"nm\", \"nm\"],\n",
    "    )\n",
    "    train_labels[train_labels.roi] = labels_data\n",
    "else:\n",
    "    train_raw = open_ds(\"cremi.zarr/train/raw\")\n",
    "    train_labels = open_ds(\"cremi.zarr/train/labels\")\n",
    "    test_raw = open_ds(\"cremi.zarr/test/raw\")\n",
    "    test_labels = open_ds(\"cremi.zarr/test/labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18be3d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Lets visualize our train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9eb01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a custom label color map for showing instances\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Create a custom label color map for showing instances\n",
    "np.random.seed(1)\n",
    "colors = [[0, 0, 0]] + [list(np.random.choice(range(256), size=3)) for _ in range(255)]\n",
    "label_cmap = ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cd695",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c001f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ims = []\n",
    "for i, (x, y) in enumerate(zip(train_raw.data, train_labels.data)):\n",
    "    # Show the raw data\n",
    "    if i == 0:\n",
    "        im = axes[0].imshow(x)\n",
    "        axes[0].set_title(\"Raw Train Data\")\n",
    "        im2 = axes[1].imshow(\n",
    "            y % 256, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\"\n",
    "        )\n",
    "        axes[1].set_title(\"Train Labels\")\n",
    "    else:\n",
    "        im = axes[0].imshow(x, animated=True)\n",
    "        im2 = axes[1].imshow(\n",
    "            y % 256,\n",
    "            cmap=label_cmap,\n",
    "            vmin=0,\n",
    "            vmax=255,\n",
    "            animated=True,\n",
    "            interpolation=\"none\",\n",
    "        )\n",
    "    ims.append([im, im2])\n",
    "\n",
    "ims = ims + ims[::-1]\n",
    "ani = animation.ArtistAnimation(fig, ims, blit=True, repeat_delay=1000)\n",
    "ani.save(\"_static/minimal_tutorial/training-data.gif\", writer=\"pillow\", fps=10)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb4aba",
   "metadata": {},
   "source": [
    "Here we visualize the training data:\n",
    "![training-data](_static/minimal_tutorial/training-data.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f0dd8",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ims = []\n",
    "for i, (x, y) in enumerate(zip(test_raw.data, test_labels.data)):\n",
    "    if i == 0:\n",
    "        im = axes[0].imshow(x)\n",
    "        axes[0].set_title(\"Raw Test Data\")\n",
    "        im2 = axes[1].imshow(\n",
    "            y % 256, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\"\n",
    "        )\n",
    "        axes[1].set_title(\"Test Labels\")\n",
    "    else:\n",
    "        im = axes[0].imshow(x, animated=True)\n",
    "        im2 = axes[1].imshow(\n",
    "            y % 256,\n",
    "            cmap=label_cmap,\n",
    "            vmin=0,\n",
    "            vmax=255,\n",
    "            animated=True,\n",
    "            interpolation=\"none\",\n",
    "        )\n",
    "    ims.append([im, im2])\n",
    "\n",
    "ims = ims + ims[::-1]\n",
    "ani = animation.ArtistAnimation(fig, ims, blit=True, repeat_delay=1000)\n",
    "ani.save(\"_static/minimal_tutorial/test-data.gif\", writer=\"pillow\", fps=10)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef867b71",
   "metadata": {},
   "source": [
    "Here we visualize the test data:\n",
    "![test-data](_static/minimal_tutorial/test-data.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770f4b5",
   "metadata": {},
   "source": [
    "### DaCapo\n",
    "Now that we have some data, lets look at how we can use DaCapo to interface with it for some common ML use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9523c5",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "We always want to be explicit when we define our data split for training and validation so that we are aware what data is being used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5cefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacapo_toolbox.datasplits import SimpleDataSplitConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasplit = SimpleDataSplitConfig(\n",
    "    name=\"cremi\",\n",
    "    path=\"cremi.zarr\",\n",
    ")\n",
    "print(f\"Train datasets: {datasplit.train}\")\n",
    "print(f\"Validation datasets: {datasplit.validate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7bc7b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Augmentation\n",
    "We almost always want to use rotations when training in EM data. This is because the structures we care about rarely have strict orientations relative to the zyx axes. However because we usually some axial anisotropy in our data, we want to be careful about how we apply these rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacapo_toolbox.trainers import GunpowderTrainerConfig\n",
    "from dacapo_toolbox.trainers.gp_augments import ElasticAugmentConfig\n",
    "\n",
    "# build a trainer config with elastic deformations accounting for the anisotropy\n",
    "trainer = GunpowderTrainerConfig(\n",
    "    name=\"rotations\",\n",
    "    augments=[\n",
    "        ElasticAugmentConfig(\n",
    "            control_point_spacing=(2, 20, 20),\n",
    "            control_point_displacement_sigma=(2, 20, 20),\n",
    "            rotation_interval=(0, 3.14),\n",
    "            subsample=4,\n",
    "            uniform_3d_rotation=False,  # rotate only in 2D\n",
    "            augmentation_probability=0.5,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b73507",
   "metadata": {},
   "source": [
    "### Simple Training loop\n",
    "The `Trainer` is only useful when combined with some data, but now that we have defined some data via the `DataSplitConfig` and the pipeline via the `TrainerConfig`, we can visualize a batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "z_slices = 13\n",
    "batch_size = 3\n",
    "\n",
    "torch_dataset = trainer.iterable_dataset(\n",
    "    datasets=datasplit.train,\n",
    "    input_shape=(z_slices, 128, 128),\n",
    "    output_shape=(z_slices, 128, 128),\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch_dataset, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74abacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(batch_size, 2, figsize=(12, 18))\n",
    "\n",
    "ims = []\n",
    "for zz in range(z_slices):\n",
    "    b_ims = []\n",
    "    for bb in range(batch_size):\n",
    "        b_raw = batch[\"raw\"][bb, 0, zz].numpy()\n",
    "        b_labels = batch[\"gt\"][bb, zz].numpy() % 256\n",
    "        if zz == 0:\n",
    "            im = axes[bb, 0].imshow(b_raw)\n",
    "            im2 = axes[bb, 1].imshow(\n",
    "                b_labels, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\"\n",
    "            )\n",
    "            if bb == 0:\n",
    "                axes[bb, 0].set_title(\"Sample Raw\")\n",
    "                axes[bb, 1].set_title(\"Sample Labels\")\n",
    "        else:\n",
    "            im = axes[bb, 0].imshow(b_raw, animated=True)\n",
    "            im2 = axes[bb, 1].imshow(\n",
    "                b_labels,\n",
    "                cmap=label_cmap,\n",
    "                vmin=0,\n",
    "                vmax=255,\n",
    "                animated=True,\n",
    "                interpolation=\"none\",\n",
    "            )\n",
    "        b_ims.extend([im, im2])\n",
    "    ims.append(b_ims)\n",
    "\n",
    "ims = ims + ims[::-1]\n",
    "ani = animation.ArtistAnimation(fig, ims, blit=True, repeat_delay=1000)\n",
    "ani.save(\"_static/minimal_tutorial/simple-batch.gif\", writer=\"pillow\", fps=10)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3019d77",
   "metadata": {},
   "source": [
    "Here we visualize the training data:\n",
    "![simple-batch](_static/minimal_tutorial/simple-batch.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1ad17",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "When training for instance segmentation, it is not possible to directly predict label ids since the ids have to be unique accross the full volume which is not possible to do with the local context that a UNet operates on. So instead we need to transform our labels into some intermediate representation that is both easy to predict and easy to post process. The most common method we use is a combination of [affinities](https://arxiv.org/pdf/1706.00120) with optional [lsds](https://github.com/funkelab/lsd) for prediction plus [mutex watershed](https://arxiv.org/abs/1904.12654) for post processing.\n",
    "\n",
    "Next we will define the task that encapsulates this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacapo_toolbox.tasks import AffinitiesTaskConfig\n",
    "\n",
    "affs_config = AffinitiesTaskConfig(\n",
    "    name=\"affs\",\n",
    "    neighborhood=[\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 9],\n",
    "        [0, 9, 0],\n",
    "        [0, 0, 27],\n",
    "        [0, 27, 0],\n",
    "    ],\n",
    "    # lsds=True,\n",
    ")\n",
    "\n",
    "torch_dataset = trainer.iterable_dataset(\n",
    "    datasets=datasplit.train,\n",
    "    input_shape=(z_slices, 128, 128),\n",
    "    output_shape=(z_slices, 128, 128),\n",
    "    task=affs_config,\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch_dataset, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8dbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(batch_size, 3, figsize=(18, 18))\n",
    "ims = []\n",
    "for zz in range(z_slices):\n",
    "    b_ims = []\n",
    "    for bb in range(batch_size):\n",
    "        b_raw = batch[\"raw\"][bb, 0, zz].numpy()\n",
    "        b_labels = batch[\"gt\"][bb, zz].numpy() % 256\n",
    "        b_target = batch[\"target\"][bb, [0, 5, 6], zz].numpy()\n",
    "        if zz == 0:\n",
    "            im = axes[bb, 0].imshow(b_raw)\n",
    "            im2 = axes[bb, 1].imshow(\n",
    "                b_labels, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\"\n",
    "            )\n",
    "            im3 = axes[bb, 2].imshow(b_target.transpose(1, 2, 0), interpolation=\"none\")\n",
    "            if bb == 0:\n",
    "                axes[bb, 0].set_title(\"Sample Raw\")\n",
    "                axes[bb, 1].set_title(\"Sample Labels\")\n",
    "                axes[bb, 2].set_title(\"Sample Affinities\")\n",
    "        else:\n",
    "            im = axes[bb, 0].imshow(b_raw, animated=True)\n",
    "            im2 = axes[bb, 1].imshow(\n",
    "                b_labels,\n",
    "                cmap=label_cmap,\n",
    "                vmin=0,\n",
    "                vmax=255,\n",
    "                animated=True,\n",
    "                interpolation=\"none\",\n",
    "            )\n",
    "            im3 = axes[bb, 2].imshow(\n",
    "                b_target.transpose(1, 2, 0), animated=True, interpolation=\"none\"\n",
    "            )\n",
    "        b_ims.extend([im, im2, im3])\n",
    "    ims.append(b_ims)\n",
    "\n",
    "ims = ims + ims[::-1]\n",
    "ani = animation.ArtistAnimation(fig, ims, blit=True, repeat_delay=1000)\n",
    "ani.save(\"_static/minimal_tutorial/affs-batch.gif\", writer=\"pillow\", fps=10)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9c9eb",
   "metadata": {},
   "source": [
    "Here we visualize a batch with (raw, gt, target) triplets for the affinities task:\n",
    "![affs-batch](_static/minimal_tutorial/affs-batch.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b027c",
   "metadata": {},
   "source": [
    "### Models\n",
    "DaCapo lets you easily train any model you want, with a special wrapper for UNets specifically. Lets make one now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacapo_toolbox.architectures import CNNectomeUNetConfig\n",
    "from funlib.geometry import Coordinate, Roi\n",
    "\n",
    "input_shape = Coordinate((5, 156, 156))\n",
    "\n",
    "unet_config = CNNectomeUNetConfig(\n",
    "    name=\"2.5D_UNet\",\n",
    "    input_shape=input_shape,\n",
    "    fmaps_in=1,\n",
    "    fmaps_out=32,\n",
    "    num_fmaps=32,\n",
    "    fmap_inc_factor=4,\n",
    "    downsample_factors=[(1, 2, 2), (1, 2, 2), (1, 2, 2)],\n",
    "    kernel_size_down=[\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "    ],\n",
    "    kernel_size_up=[\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "        [(1, 3, 3), (1, 3, 3)],\n",
    "        [(3, 3, 3), (3, 3, 3)],\n",
    "    ],\n",
    ")\n",
    "\n",
    "output_shape = unet_config.compute_output_shape(input_shape)\n",
    "print(f\"Given an input of shape {input_shape} we get an out of shape {output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7aa030",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "Now we can bring everything together and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f587dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = trainer.iterable_dataset(\n",
    "    datasets=datasplit.train,\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    task=affs_config,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=3,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "\n",
    "task = affs_config.task_type(affs_config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# this ensures we output the appropriate number of channels, use the appropriate final activation etc.\n",
    "module = task.create_model(unet_config).to(device)\n",
    "loss = task.loss\n",
    "optimizer = torch.optim.Adam(module.parameters(), lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for iteration, batch in enumerate(iter(dataloader)):\n",
    "    raw, target, weight = (\n",
    "        batch[\"raw\"].to(device),\n",
    "        batch[\"target\"].to(device),\n",
    "        batch[\"weight\"].to(device),\n",
    "    )\n",
    "    optimizer.zero_grad()\n",
    "    output = module(raw)\n",
    "    loss_value = loss.compute(output, target, weight)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss_value.item())\n",
    "\n",
    "    if iteration >= 800:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e164f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79688cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwatershed as mws\n",
    "\n",
    "# Lets predict on some validation data:\n",
    "val_raw, val_gt = datasplit.validate[0].raw, datasplit.validate[0].gt\n",
    "# fetch a xy slice from the center of our validation volume\n",
    "# We snap to grid to a multiple of the max downsampling factor of\n",
    "# the unet (1, 8, 8) to ensure downsampling is always possible\n",
    "roi = val_raw.roi\n",
    "z_coord = Coordinate(1, 0, 0)\n",
    "xy_coord = Coordinate(0, 1, 1)\n",
    "center_offset = roi.center * z_coord + roi.offset * xy_coord + (roi.shape * xy_coord) // 4\n",
    "center_size = val_raw.voxel_size * z_coord + (roi.shape * xy_coord) // 2\n",
    "center_slice = Roi(center_offset, center_size)\n",
    "center_slice = center_slice.snap_to_grid(val_raw.voxel_size * Coordinate(1, 8, 8))\n",
    "context = (input_shape - output_shape) // 2 * val_raw.voxel_size\n",
    "\n",
    "# Read the raw data\n",
    "raw_input = val_raw.to_ndarray(center_slice.grow(context, context))\n",
    "\n",
    "# Predict on the validation data\n",
    "with torch.no_grad():\n",
    "    device = torch.device(\"cpu\")\n",
    "    module = module.to(device)\n",
    "    pred = (\n",
    "        module(torch.from_numpy(raw_input).to(device).unsqueeze(0).unsqueeze(0))\n",
    "        .cpu()\n",
    "        .detach()\n",
    "        .numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53884e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 8))\n",
    "padding = (input_shape - output_shape) // 2\n",
    "\n",
    "# select the long range affinity channels for visualization\n",
    "prediction = pred[0, [0, 5, 6], 0]\n",
    "\n",
    "# Run mutex watershed on the affinity predictions.\n",
    "# We subtract 0.5 to move affs from range (0, 1) to (-0.5, 0.5).\n",
    "# This is because mutex only splits objects on negative edges.\n",
    "pred_labels = (\n",
    "    mws.agglom(pred[0].astype(np.float64) - 0.5, offsets=affs_config.neighborhood)[0]\n",
    "    % 256\n",
    ")\n",
    "\n",
    "# read the ground truth labels\n",
    "gt_labels = val_gt.to_ndarray(center_slice)[0] % 256\n",
    "\n",
    "# Pad\n",
    "prediction = np.pad(\n",
    "    prediction,\n",
    "    ((0,), (padding[1],), (padding[2],)),\n",
    "    mode=\"constant\",\n",
    "    constant_values=np.nan,\n",
    ")\n",
    "pred_labels = np.pad(\n",
    "    pred_labels,\n",
    "    ((padding[1],), (padding[2],)),\n",
    "    mode=\"constant\",\n",
    "    constant_values=0,\n",
    ")\n",
    "gt_labels = np.pad(\n",
    "    gt_labels,\n",
    "    ((padding[1],), (padding[2],)),\n",
    "    mode=\"constant\",\n",
    "    constant_values=0,\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "im_raw = axes[0].imshow(raw_input[2])\n",
    "im2 = axes[1].imshow(gt_labels, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\")\n",
    "im4 = axes[2].imshow(prediction.transpose(1, 2, 0), interpolation=\"none\")\n",
    "im5 = axes[3].imshow(\n",
    "    pred_labels, cmap=label_cmap, vmin=0, vmax=255, interpolation=\"none\"\n",
    ")\n",
    "axes[0].set_title(\"Val Raw\")\n",
    "axes[1].set_title(\"Val Labels\")\n",
    "axes[2].set_title(\"Pred Affinities\")\n",
    "axes[3].set_title(\"Pred Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054b777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
